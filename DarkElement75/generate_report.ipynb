{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#For now we don't do anything with standards since we don't have a standards key file\n",
    "\n",
    "#Only read in these columns\n",
    "data = pd.read_csv(\"GenoResults.csv\", usecols=[\"DNA_PLATE\", \"WELL\", \"SAMPLE_ID\", \"PLANT_ID\", \"LINE_NM\", \"MARKER_NM\", \"Call\"])\n",
    "\n",
    "#Remove all rows with empty entries\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get unique column values\n",
    "marker_nms = np.sort(np.unique(data[\"MARKER_NM\"]))\n",
    "sample_ids = np.unique(data[\"SAMPLE_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort so that we group the same samples together, as well as order them.\n",
    "data = data.sort_values([\"SAMPLE_ID\", \"PLANT_ID\", \"MARKER_NM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add columns for each marker_nm value\n",
    "for marker_nm in marker_nms:\n",
    "    data[marker_nm] = \"\"\n",
    "\n",
    "\"\"\"\n",
    "We have groups of which share the same line_nm and plant_id in this table. \n",
    "\n",
    "For each of these groups, we collapse them so that each row's unique call value is put in the just-created columns\n",
    "    for each marker_nm value. E.g. for rows that have 4:AA, 3:BB, 5:CC where 3, 4, 5 are marker_nm values\n",
    "    and AA, BB, and CC are call values, we'd then combine these so that we had one result row like this:\n",
    "    \n",
    "    4  |  3  |  5\n",
    "    AA    BB    CC\n",
    "    \n",
    "    Where 4, 3, and 5 are the new headings. So we're taking these call values from the related segments \n",
    "        and condensing theminto one row for each of these segments.\n",
    "        \n",
    "Though the segments may all be the same length as the number of marker_nm values, e.g. in the above example this\n",
    "    length was 3, this isn't always the case, so we iterate through each row and know we're done with \n",
    "    the segment once we've filled out all the values. Once we're done with this segment, we increment the index\n",
    "    for the destination row.\n",
    "\"\"\"\n",
    "dst = 0\n",
    "marker_nm_loc = data.columns.get_loc(\"MARKER_NM\")\n",
    "call_loc = data.columns.get_loc(\"Call\")\n",
    "plant_id_loc = data.columns.get_loc(\"PLANT_ID\")\n",
    "well_loc = data.columns.get_loc(\"WELL\")\n",
    "\n",
    "#Loop through all individual rows\n",
    "for src in range(len(data)):\n",
    "    \n",
    "    #If there are not any empty elements in the dst row, then we're done and can move on \n",
    "    if not np.any(data.iloc[dst] == \"\"):\n",
    "        #New group, increment dst row index\n",
    "        dst+=1\n",
    "        \n",
    "        #Set new row to match group info\n",
    "        data.iloc[dst] = data.iloc[src]\n",
    "        \n",
    "    #Insert new call value in corresponding marker_nm column\n",
    "    data.iat[dst, data.columns.get_loc(data.iat[src, marker_nm_loc])] = data.iat[src, call_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove all remaining original rows, i.e. the rows used for condensing after the last complete condensed row.\n",
    "if not np.any(data.iloc[dst] == \"\"):\n",
    "    #Include last row since it's complete\n",
    "    data = data[:dst]\n",
    "else:\n",
    "    #Don't include the last row since it's incomplete\n",
    "    data = data[:dst-1]\n",
    "    \n",
    "#MARKER_NM and Call rows are no longer necessary since they were used for condensing, so we remove these as well.\n",
    "data = data.drop([\"MARKER_NM\", \"Call\"], axis=1)\n",
    "\n",
    "#Create a new PROFILE column for the concatenation of the marker_nm columns.\n",
    "data[\"PROFILE\"] = data[[marker_nm for marker_nm in marker_nms]].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a mapping from PROFILE:LINE_NM using instances of each PROFILE for which PROFILE is\n",
    "    only As and Bs, i.e. the PROFILE is complete and has no errors.\n",
    "    \n",
    "For every LINE_NM entry, get the associated PROFILE if it has no errors.\n",
    "\n",
    "Since there are often several errors in PROFILE values, we have to find the true PROFILE for each LINE_NM\n",
    "    value. We do this by counting the instances of each non-error PROFILE for each LINE_NM,\n",
    "    and then obtaining the true profile via the PROFILE which occurs most often.\n",
    "    \n",
    "Since often times the counts will be similar to [893, 2, 1, 2, 1] due to most being correct, but\n",
    "    there still being a few errors, in the above case 6 errors.\n",
    "\"\"\"\n",
    "#Mapping dictionary to find the true PROFILE for each LINE_NM value.\n",
    "line_nms = {}\n",
    "\n",
    "#Create these ahead of time to avoid checks in the loop\n",
    "for line_nm in np.unique(data[\"LINE_NM\"]):\n",
    "    line_nms[line_nm] = {}\n",
    "\n",
    "def profile_is_valid(profile):\n",
    "    #Ensure profile is complete and has no errors, i.e. is only made up of A or B characters.\n",
    "    valid = True\n",
    "    for c in profile:\n",
    "        if c not in [\"A\", \"B\"]:\n",
    "            valid = False\n",
    "            break\n",
    "    return valid\n",
    "            \n",
    "for i, row in data.iterrows():\n",
    "    profile = row[\"PROFILE\"]\n",
    "    \n",
    "    #If it's not valid, move on to the next one.\n",
    "    if not profile_is_valid(profile):\n",
    "        continue\n",
    "\n",
    "    #Otherwise, increment this profile's count in its respective line_nm entry in our line_nms dictionary,\n",
    "    #setting its count to 1 if it doesn't already exist.\n",
    "    line_nm = row[\"LINE_NM\"]\n",
    "    if profile in line_nms[line_nm]:\n",
    "        line_nms[line_nm][profile] +=1\n",
    "    else:\n",
    "        line_nms[line_nm][profile] = 1\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Get the true profile for each line_nm, and insert the true profile into a new mapping dictionary,\n",
    "    made to map from profile:line_nm for each sample_id value.\n",
    "    \n",
    "Since it will also be used to count the number of each true profile (and failures) for each sample_id,\n",
    "    we also duplicate this mapping across each unique SAMPLE_ID and store a count value for each mapping as well, \n",
    "    initialized to 0.\n",
    "    \n",
    "So it will be of the form sample_id:profile:{LINE_NM:line_nm, COUNT:count}\n",
    "\"\"\"\n",
    "        \n",
    "#Mapping dictionary for all profiles and sample_ids\n",
    "profiles = {}\n",
    "\n",
    "#Add dictionaries for all unique sample_ids to avoid checks in the loop,\n",
    "#And to add an entry to count failures.\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    profiles[sample_id] = {\"FAIL\":{\"COUNT\":0}}\n",
    "    \n",
    "#Add true profile : line_nm, count mapping to every sample_id in the profiles dict.\n",
    "for line_nm,profile_counts in line_nms.items():\n",
    "    true_profile = max(profile_counts, key=(lambda key: profile_counts[key]))\n",
    "    for sample_id in sample_ids:\n",
    "        profiles[sample_id][true_profile] = {\"LINE_NM\":line_nm, \"COUNT\":0}\n",
    "\n",
    "#for each row in our data:\n",
    "for i, row in data.iterrows():\n",
    "    sample_id = row[\"SAMPLE_ID\"]\n",
    "    profile = row[\"PROFILE\"]\n",
    "    #if the profile has an error:\n",
    "    if not profile_is_valid(profile):\n",
    "        #we increment the FAIL count in the profiles dict for this row's sample_id and continue\n",
    "        profiles[sample_id][\"FAIL\"][\"COUNT\"]+=1\n",
    "        continue\n",
    "    \n",
    "    #if the profile is not in our profiles dict for this row's sample_id:\n",
    "    if profile not in profiles[sample_id]:\n",
    "        #we increment the FAIL count in the profiles dict for this row's sample_id and continue\n",
    "        profiles[sample_id][\"FAIL\"][\"COUNT\"]+=1\n",
    "        continue\n",
    "    \n",
    "    #if the line_nm the profile maps to is not the same as the line_nm for this row:\n",
    "    if profiles[sample_id][profile][\"LINE_NM\"] != row[\"LINE_NM\"]:\n",
    "        #we increment the FAIL count in the profiles dict for this row's sample_id and continue\n",
    "        profiles[sample_id][\"FAIL\"][\"COUNT\"]+=1\n",
    "        continue\n",
    "        \n",
    "    #Otherwise, we increment the count in the profiles dict for this profile and sample id\n",
    "    profiles[sample_id][profile][\"COUNT\"]+=1\n",
    "    \n",
    "#Get percentages of each COUNT relative to the total counts for each sample_id, \n",
    "#    and add this entry alongside each COUNT\n",
    "for sample_id in sample_ids:\n",
    "    #Get sum of counts in this sample_id\n",
    "    count_sum = 0\n",
    "    for profile in profiles[sample_id]:\n",
    "        count_sum += profiles[sample_id][profile][\"COUNT\"]\n",
    "    \n",
    "    #Use sum of counts to add PERCENTAGE attribute\n",
    "    for profile in profiles[sample_id]:\n",
    "        profiles[sample_id][profile][\"PERCENTAGE\"] = profiles[sample_id][profile][\"COUNT\"]/count_sum*100\n",
    "        \n",
    "#Generate a report dataframe with our complete profiles dictionary\n",
    "report = pd.DataFrame(columns=[\"SAMPLE_ID\", \"LINE_NM\", \"PROFILE\", \"COUNT\", \"PERCENTAGE\"])\n",
    "for sample_id in sample_ids:\n",
    "    for profile,profile_data in sorted(profiles[sample_id].items()):\n",
    "        report = report.append({\"SAMPLE_ID\":sample_id, \n",
    "                       \"LINE_NM\":profile_data[\"LINE_NM\"] if \"LINE_NM\" in profile_data else \"FAIL\", \n",
    "                       \"PROFILE\":profile, \n",
    "                       \"COUNT\":profile_data[\"COUNT\"], \n",
    "                       \"PERCENTAGE\":profile_data[\"PERCENTAGE\"]}, ignore_index=True)\n",
    "        \n",
    "#Order the columns for data csv before writing\n",
    "data_cols = [\"SAMPLE_ID\", \"DNA_PLATE\", \"WELL\", \"PLANT_ID\", \"LINE_NM\"]\n",
    "data_cols.extend([marker_nm for marker_nm in marker_nms])\n",
    "data_cols.append(\"PROFILE\")\n",
    "\n",
    "#With both our report dataframe and condensed data dataframe finished, we write both to csvs.\n",
    "report.to_csv(\"automated_report.csv\", index=False)\n",
    "data.to_csv(\"automated_supplemental_data.csv\", columns=data_cols, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
